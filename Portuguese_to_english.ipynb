{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "papermill": {
      "duration": 978.415403,
      "end_time": "2021-01-21T13:49:38.569102",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-21T13:33:20.153699",
      "version": "2.1.0"
    },
    "colab": {
      "name": "Portuguese to english.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzRcpO04oCW"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:27.576443Z",
          "iopub.status.busy": "2021-01-21T13:33:27.575716Z",
          "iopub.status.idle": "2021-01-21T13:33:33.308759Z",
          "shell.execute_reply": "2021-01-21T13:33:33.307768Z"
        },
        "papermill": {
          "duration": 5.749362,
          "end_time": "2021-01-21T13:33:33.308907",
          "exception": false,
          "start_time": "2021-01-21T13:33:27.559545",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv76NoqX4oCX",
        "outputId": "0ffefefa-d8ee-4a6a-dd33-40aaa21d9bcc"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "smoothing = SmoothingFunction().method4\n",
        "!wget http://www.manythings.org/anki/por-eng.zip\n",
        "!unzip ./por-eng.zip\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-09 02:03:08--  http://www.manythings.org/anki/por-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.21.55.222, 2606:4700:3031::6815:37de, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5868438 (5.6M) [application/zip]\n",
            "Saving to: ‘por-eng.zip’\n",
            "\n",
            "por-eng.zip         100%[===================>]   5.60M  4.18MB/s    in 1.3s    \n",
            "\n",
            "2021-06-09 02:03:10 (4.18 MB/s) - ‘por-eng.zip’ saved [5868438/5868438]\n",
            "\n",
            "Archive:  ./por-eng.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: _about.txt              \n",
            "  inflating: por.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj6Kqp_k4oCX"
      },
      "source": [
        "### Data cleaning and train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.345280Z",
          "iopub.status.busy": "2021-01-21T13:33:33.343279Z",
          "iopub.status.idle": "2021-01-21T13:33:33.621266Z",
          "shell.execute_reply": "2021-01-21T13:33:33.620256Z"
        },
        "papermill": {
          "duration": 0.301099,
          "end_time": "2021-01-21T13:33:33.621391",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.320292",
          "status": "completed"
        },
        "tags": [],
        "id": "LaapE2Ud4oCY"
      },
      "source": [
        "data_path = '/content/por.txt' \n",
        "num_sentences = 20000 \n",
        "\n",
        "# opening the text file and getting the data \n",
        "with open(data_path,'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "#count the number of sentences\n",
        "c=0 \n",
        "\n",
        "# data cleaning\n",
        "source_texts,target_texts = [],[]\n",
        "for line in lines: # going through each lines\n",
        "    if c == num_sentences: # if we have 20000 sentences than we will get out of this loop\n",
        "        break \n",
        "    elif '\\t' in line:\n",
        "        op_data,ip_data,_ = line.lower().rstrip().split('\\t') # lowering the data and then spliting the data\n",
        "        # to remove the punctuation we did not include last character\n",
        "        source_text = ip_data[:-1].strip()\n",
        "        target_text = op_data[:-1].strip()\n",
        "        # removing the unprintable character\n",
        "        # for english and french we will take anly alphabets of brespective languages and numbers\n",
        "        target_text = re.sub(\"[^a-z 1-9\\'-]\",\"\",target_text) \n",
        "        source_text = re.sub(\"[^a-zàâãçéèêëîïôœùûüÿ 1-9\\'-]\",\"\",source_text) \n",
        "        source_texts.append(source_text)\n",
        "        target_texts.append(target_text)\n",
        "        c+=1\n",
        "\n",
        "# train_test_split of the source and target data\n",
        "source_train,source_test,target_train,target_test = train_test_split(source_texts,target_texts,test_size = 0.2, random_state= 0)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPvULJAh4oCY"
      },
      "source": [
        "### Making the required functions for the data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.652187Z",
          "iopub.status.busy": "2021-01-21T13:33:33.650331Z",
          "iopub.status.idle": "2021-01-21T13:33:33.652845Z",
          "shell.execute_reply": "2021-01-21T13:33:33.653254Z"
        },
        "papermill": {
          "duration": 0.021365,
          "end_time": "2021-01-21T13:33:33.653354",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.631989",
          "status": "completed"
        },
        "tags": [],
        "id": "YjFYZtBD4oCZ"
      },
      "source": [
        "# tokenizer for data\n",
        "def create_tokenizer(texts):\n",
        "    tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "# one_hot encoding of the target data\n",
        "def one_hot(pad_seq,max_sent_length,num_vocab):\n",
        "    target_data_one_hot = np.zeros((len(pad_seq),max_sent_length,num_vocab))\n",
        "    for i,w in enumerate(pad_seq):\n",
        "        for j,d in enumerate(w):\n",
        "            target_data_one_hot[i,j,d] = 1\n",
        "    return target_data_one_hot\n",
        "\n",
        "# for padding the data\n",
        "def encoding_text(tokenizer,text,max_length):\n",
        "    text_seq = tokenizer.texts_to_sequences(text)\n",
        "    pad_seq = pad_sequences(text_seq,maxlen= max_length)\n",
        "    return pad_seq\n",
        "\n",
        "# to find the maximum length of the sentence from data\n",
        "def max_length(text):\n",
        "    return max(len(l.split()) for l in text)\n",
        "    "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsB6Npw64oCZ"
      },
      "source": [
        "### Preparing training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxAtfl57FZR",
        "outputId": "67d27434-f054-4a62-a11b-826091effb36"
      },
      "source": [
        "# preparing source tokenizer and getting relevant information\n",
        "source_tokenizer = create_tokenizer(source_train)\n",
        "source_vocab = source_tokenizer.word_index\n",
        "num_source_vocab = len(source_vocab)+1\n",
        "max_source_length = max_length(source_train)\n",
        "print(\"Number of Source Vocabulary :\",num_source_vocab)\n",
        "print(\"Maximum Source Length :\",max_source_length)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Source Vocabulary : 5029\n",
            "Maximum Source Length : 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLrtEWly7FW4",
        "outputId": "607f0c0a-6df4-4d92-a2c3-db6f41c5fe88"
      },
      "source": [
        "# preparing target tokenizer and getting relevant information\n",
        "target_tokenizer = create_tokenizer(target_train)\n",
        "target_vocab = target_tokenizer.word_index\n",
        "num_target_vocab = len(target_vocab)+1\n",
        "max_target_length = max_length(target_train)\n",
        "print(\"Number of Target Vocabulary :\",num_target_vocab)\n",
        "print(\"Maximum Target Length :\",max_target_length)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Target Vocabulary : 3070\n",
            "Maximum Target Length : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtEbpJMf7FUb",
        "outputId": "658b1432-5641-492d-ebfe-c50be3fa6255"
      },
      "source": [
        "# preparing the training data\n",
        "# padding of the source sentences\n",
        "source_train_seq_pad = encoding_text(source_tokenizer,source_train,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_train_seq_pad = encoding_text(target_tokenizer,target_train,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_train_seq_pad = one_hot(target_train_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded train source\n",
            "[[   0    0    0 ...   11    4 2837]\n",
            " [   0    0    0 ...    3   40 2838]\n",
            " [   0    0    0 ...   17   13  368]\n",
            " ...\n",
            " [   0    0    0 ...    4   48    2]\n",
            " [   0    0    0 ...  289   22  227]\n",
            " [   0    0    0 ...    3    8  171]]\n",
            "-------------------------------------\n",
            "Padded train target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.696956Z",
          "iopub.status.busy": "2021-01-21T13:33:33.691743Z",
          "iopub.status.idle": "2021-01-21T13:33:34.882871Z",
          "shell.execute_reply": "2021-01-21T13:33:34.883337Z"
        },
        "papermill": {
          "duration": 1.219756,
          "end_time": "2021-01-21T13:33:34.883461",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.663705",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ih4M5QQ4oCZ",
        "outputId": "babf5551-726c-4532-e74b-e44383969171"
      },
      "source": [
        "# preparing the test data\n",
        "# padding of the source sentences\n",
        "source_test_seq_pad = encoding_text(source_tokenizer,source_test,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_test_seq_pad = encoding_text(target_tokenizer,target_test,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_test_seq_pad = one_hot(target_test_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded test source\n",
            "[[   0    0    0 ...   11    4 2837]\n",
            " [   0    0    0 ...    3   40 2838]\n",
            " [   0    0    0 ...   17   13  368]\n",
            " ...\n",
            " [   0    0    0 ...    4   48    2]\n",
            " [   0    0    0 ...  289   22  227]\n",
            " [   0    0    0 ...    3    8  171]]\n",
            "-------------------------------------\n",
            "Padded test target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMJ0uDVW4oCa"
      },
      "source": [
        "### Preparing and running the Autoencoder model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:34.914566Z",
          "iopub.status.busy": "2021-01-21T13:33:34.914052Z",
          "iopub.status.idle": "2021-01-21T13:36:55.385009Z",
          "shell.execute_reply": "2021-01-21T13:36:55.384379Z"
        },
        "papermill": {
          "duration": 200.490724,
          "end_time": "2021-01-21T13:36:55.385175",
          "exception": false,
          "start_time": "2021-01-21T13:33:34.894451",
          "status": "completed"
        },
        "scrolled": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bLmntXQ4oCa",
        "outputId": "f1153ef6-d975-4404-eb03-1edd46a08a83"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(max_source_length,)))\n",
        "model.add(Embedding(num_source_vocab,512,mask_zero=True))\n",
        "model.add(LSTM(100,return_sequences = False))\n",
        "model.add(RepeatVector(max_target_length))\n",
        "model.add(LSTM(100,return_sequences = True))\n",
        "model.add(TimeDistributed(Dense(num_target_vocab,activation = 'softmax')))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 8, 512)            2574848   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 100)               245200    \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 5, 100)            0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 5, 100)            80400     \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 5, 3070)           310070    \n",
            "=================================================================\n",
            "Total params: 3,210,518\n",
            "Trainable params: 3,210,518\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LkVZFgi8vLa"
      },
      "source": [
        "es = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01)\n",
        "filepath = './portuguese-english.h5' \n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4PbSust84ba",
        "outputId": "5594746b-b276-4ffe-ac0e-c92bc4d079ee"
      },
      "source": [
        "history = model.fit(source_train_seq_pad, target_train_seq_pad, epochs= 100, batch_size=64, validation_data = (source_test_seq_pad,target_test_seq_pad), verbose=1,callbacks=[checkpoint,es])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 11s 29ms/step - loss: 4.4661 - acc: 0.3733 - val_loss: 3.9629 - val_acc: 0.3747\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.37465, saving model to ./portuguese-english.h5\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 3.7925 - acc: 0.3877 - val_loss: 3.7155 - val_acc: 0.3905\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.37465 to 0.39050, saving model to ./portuguese-english.h5\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 3.5668 - acc: 0.3959 - val_loss: 3.5159 - val_acc: 0.3991\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.39050 to 0.39910, saving model to ./portuguese-english.h5\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 3.3610 - acc: 0.4196 - val_loss: 3.3332 - val_acc: 0.4297\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.39910 to 0.42975, saving model to ./portuguese-english.h5\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 3.1338 - acc: 0.4554 - val_loss: 3.1206 - val_acc: 0.4630\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.42975 to 0.46300, saving model to ./portuguese-english.h5\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.9271 - acc: 0.4843 - val_loss: 2.9696 - val_acc: 0.4812\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.46300 to 0.48120, saving model to ./portuguese-english.h5\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.7527 - acc: 0.5056 - val_loss: 2.8406 - val_acc: 0.5019\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.48120 to 0.50185, saving model to ./portuguese-english.h5\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.5999 - acc: 0.5274 - val_loss: 2.7260 - val_acc: 0.5221\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.50185 to 0.52205, saving model to ./portuguese-english.h5\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.4606 - acc: 0.5497 - val_loss: 2.6279 - val_acc: 0.5343\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.52205 to 0.53425, saving model to ./portuguese-english.h5\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.3358 - acc: 0.5683 - val_loss: 2.5306 - val_acc: 0.5520\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.53425 to 0.55200, saving model to ./portuguese-english.h5\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.2319 - acc: 0.5831 - val_loss: 2.4668 - val_acc: 0.5610\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.55200 to 0.56105, saving model to ./portuguese-english.h5\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.1384 - acc: 0.5992 - val_loss: 2.4141 - val_acc: 0.5685\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.56105 to 0.56850, saving model to ./portuguese-english.h5\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.0472 - acc: 0.6130 - val_loss: 2.3649 - val_acc: 0.5785\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.56850 to 0.57850, saving model to ./portuguese-english.h5\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.9620 - acc: 0.6269 - val_loss: 2.3182 - val_acc: 0.5863\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.57850 to 0.58625, saving model to ./portuguese-english.h5\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.8753 - acc: 0.6414 - val_loss: 2.2809 - val_acc: 0.5919\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.58625 to 0.59190, saving model to ./portuguese-english.h5\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.7938 - acc: 0.6553 - val_loss: 2.2464 - val_acc: 0.6005\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.59190 to 0.60050, saving model to ./portuguese-english.h5\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.7176 - acc: 0.6688 - val_loss: 2.1921 - val_acc: 0.6079\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.60050 to 0.60795, saving model to ./portuguese-english.h5\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.6485 - acc: 0.6818 - val_loss: 2.1580 - val_acc: 0.6160\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.60795 to 0.61600, saving model to ./portuguese-english.h5\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.5800 - acc: 0.6948 - val_loss: 2.1246 - val_acc: 0.6231\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.61600 to 0.62315, saving model to ./portuguese-english.h5\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.5159 - acc: 0.7068 - val_loss: 2.1091 - val_acc: 0.6214\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.62315\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.4596 - acc: 0.7189 - val_loss: 2.0782 - val_acc: 0.6308\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.62315 to 0.63080, saving model to ./portuguese-english.h5\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.4090 - acc: 0.7312 - val_loss: 2.0789 - val_acc: 0.6353\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.63080 to 0.63535, saving model to ./portuguese-english.h5\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.3615 - acc: 0.7403 - val_loss: 2.0390 - val_acc: 0.6390\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.63535 to 0.63900, saving model to ./portuguese-english.h5\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.3152 - acc: 0.7501 - val_loss: 2.0396 - val_acc: 0.6463\n",
            "\n",
            "Epoch 00024: val_acc improved from 0.63900 to 0.64630, saving model to ./portuguese-english.h5\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.2633 - acc: 0.7613 - val_loss: 2.0150 - val_acc: 0.6493\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.64630 to 0.64930, saving model to ./portuguese-english.h5\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.2093 - acc: 0.7705 - val_loss: 1.9960 - val_acc: 0.6506\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.64930 to 0.65060, saving model to ./portuguese-english.h5\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.1649 - acc: 0.7811 - val_loss: 1.9783 - val_acc: 0.6544\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.65060 to 0.65445, saving model to ./portuguese-english.h5\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.1269 - acc: 0.7894 - val_loss: 1.9637 - val_acc: 0.6593\n",
            "\n",
            "Epoch 00028: val_acc improved from 0.65445 to 0.65930, saving model to ./portuguese-english.h5\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.0939 - acc: 0.7960 - val_loss: 1.9506 - val_acc: 0.6614\n",
            "\n",
            "Epoch 00029: val_acc improved from 0.65930 to 0.66135, saving model to ./portuguese-english.h5\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.0601 - acc: 0.8038 - val_loss: 1.9543 - val_acc: 0.6605\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.66135\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.0246 - acc: 0.8103 - val_loss: 1.9533 - val_acc: 0.6666\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.66135 to 0.66660, saving model to ./portuguese-english.h5\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.9858 - acc: 0.8173 - val_loss: 1.9483 - val_acc: 0.6611\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.66660\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.9478 - acc: 0.8220 - val_loss: 1.9353 - val_acc: 0.6686\n",
            "\n",
            "Epoch 00033: val_acc improved from 0.66660 to 0.66865, saving model to ./portuguese-english.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:57.032153Z",
          "iopub.status.busy": "2021-01-21T13:36:57.031063Z",
          "iopub.status.idle": "2021-01-21T13:36:57.047282Z",
          "shell.execute_reply": "2021-01-21T13:36:57.046676Z"
        },
        "papermill": {
          "duration": 0.844983,
          "end_time": "2021-01-21T13:36:57.047395",
          "exception": false,
          "start_time": "2021-01-21T13:36:56.202412",
          "status": "completed"
        },
        "tags": [],
        "id": "jiNVZlvP4oCb"
      },
      "source": [
        "# loading the weights from the best saved model\n",
        "model.load_weights(filepath)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFknwSI64oCb"
      },
      "source": [
        "### Making the functions to predict the sequence and BLEU_sccore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:58.701542Z",
          "iopub.status.busy": "2021-01-21T13:36:58.699715Z",
          "iopub.status.idle": "2021-01-21T13:36:58.702231Z",
          "shell.execute_reply": "2021-01-21T13:36:58.702636Z"
        },
        "papermill": {
          "duration": 0.838898,
          "end_time": "2021-01-21T13:36:58.702743",
          "exception": false,
          "start_time": "2021-01-21T13:36:57.863845",
          "status": "completed"
        },
        "tags": [],
        "id": "N6QTt-LU4oCb"
      },
      "source": [
        "# a dictionary having key is a token number for a particular word and value is a word\n",
        "# this will required to decode the predicted sequence\n",
        "target_vocab_idx = {v:k for k,v in target_tokenizer.word_index.items()}\n",
        "\n",
        "# function to predict the decoded sequence\n",
        "def predict_sequence(model,sent,vocab_idx):\n",
        "    prediction = model.predict(sent.reshape(1,max_source_length))[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = []\n",
        "    for i in integers:\n",
        "        if i != 0:\n",
        "            word = vocab_idx[i]\n",
        "            if word is None:\n",
        "                break\n",
        "            target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "# for evaluation of the model through BLEU_score\n",
        "def bleu_score(model,ip,ip_raw,op_raw,vocab_idx):\n",
        "    prediction,actual = [],[]\n",
        "    for i,sent in enumerate(ip):\n",
        "        if i%10 == 0:\n",
        "            print('\\rprogress ',(i+1)*100//len(ip),'%',sep='',end='',flush = True)\n",
        "        translation = predict_sequence(model,sent,vocab_idx)\n",
        "        prediction.append(translation)\n",
        "        actual.append(op_raw[i])\n",
        "    print()\n",
        "    # printing the first ten sentences\n",
        "    for i in range(10):\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "        print('PORTUGUESE -->',ip_raw[i],' || ','ACTUAL ENGLISH -->',op_raw[i],' || ','PREDICTED ENGLISH -->',prediction[i])\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print()\n",
        "    # printing the BLEU_score\n",
        "    print(\"----------------------------------\")\n",
        "    print('Printing BLEU SCORE...')\n",
        "    print(\"----------------------------------\")\n",
        "    print('First BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(1.0, 0, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Second BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Third BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Fourth BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print(\"----------------------------------\")"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPcAEs24oCc"
      },
      "source": [
        "### Evaluating the model on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:37:00.637318Z",
          "iopub.status.busy": "2021-01-21T13:37:00.636492Z",
          "iopub.status.idle": "2021-01-21T13:47:02.656690Z",
          "shell.execute_reply": "2021-01-21T13:47:02.656212Z"
        },
        "papermill": {
          "duration": 603.117053,
          "end_time": "2021-01-21T13:47:02.656787",
          "exception": false,
          "start_time": "2021-01-21T13:36:59.539734",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2KUCpGO4oCc",
        "outputId": "e0ac43a2-7485-4a59-dd77-a09deb0ae8a3"
      },
      "source": [
        "bleu_score(model,source_train_seq_pad,source_train,target_train,target_vocab_idx)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> isso é vergonhoso  ||  ACTUAL ENGLISH --> this is shameful  ||  PREDICTED ENGLISH --> this is sweet\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> tom foi promovido  ||  ACTUAL ENGLISH --> tom was promoted  ||  PREDICTED ENGLISH --> tom was ok\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> eu sou um garoto  ||  ACTUAL ENGLISH --> i am a boy  ||  PREDICTED ENGLISH --> am a boy\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> você terminou  ||  ACTUAL ENGLISH --> are you finished  ||  PREDICTED ENGLISH --> are you finished\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> ns receberemos um  ||  ACTUAL ENGLISH --> we'll get one  ||  PREDICTED ENGLISH --> we'll get one\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> me estou em casa  ||  ACTUAL ENGLISH --> mom i'm home  ||  PREDICTED ENGLISH --> i'm i'm home\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> chame o seu irmão  ||  ACTUAL ENGLISH --> call your brother  ||  PREDICTED ENGLISH --> call your depressed\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> você se sente sortuda  ||  ACTUAL ENGLISH --> do you feel lucky  ||  PREDICTED ENGLISH --> do you feel lucky\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> sou muito curioso  ||  ACTUAL ENGLISH --> i'm very curious  ||  PREDICTED ENGLISH --> i'm very curious\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> estou legal  ||  ACTUAL ENGLISH --> i'm all right  ||  PREDICTED ENGLISH --> am am done\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.637352\n",
            "Second BLEU score --> 0.448290\n",
            "Third BLEU score --> 0.402577\n",
            "Fourth BLEU score --> 0.310673\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYxGyiZ4oCd"
      },
      "source": [
        "### Evaluating the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:47:05.406595Z",
          "iopub.status.busy": "2021-01-21T13:47:05.405722Z",
          "iopub.status.idle": "2021-01-21T13:49:35.405960Z",
          "shell.execute_reply": "2021-01-21T13:49:35.405286Z"
        },
        "papermill": {
          "duration": 151.514089,
          "end_time": "2021-01-21T13:49:35.406089",
          "exception": false,
          "start_time": "2021-01-21T13:47:03.892000",
          "status": "completed"
        },
        "tags": [],
        "id": "oqw6Q6Fb4oCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273176df-0bc7-412d-c0a6-a6fb45fcda24"
      },
      "source": [
        "bleu_score(model,source_test_seq_pad,source_test,target_test,target_vocab_idx)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> vocês não estão erradas  ||  ACTUAL ENGLISH --> you aren't wrong  ||  PREDICTED ENGLISH --> you aren't sleepy\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> estamos melhor  ||  ACTUAL ENGLISH --> we're better  ||  PREDICTED ENGLISH --> we're better\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> tom est em boston  ||  ACTUAL ENGLISH --> is tom in boston  ||  PREDICTED ENGLISH --> is is in boston\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> vocês j pagaram  ||  ACTUAL ENGLISH --> you already paid  ||  PREDICTED ENGLISH --> you you kill\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> o tom respondeu  ||  ACTUAL ENGLISH --> did tom reply  ||  PREDICTED ENGLISH --> tom did\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> eu ronco  ||  ACTUAL ENGLISH --> i snore  ||  PREDICTED ENGLISH --> i i i\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> tom é feliz  ||  ACTUAL ENGLISH --> tom's happy  ||  PREDICTED ENGLISH --> tom is happy\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> encontrei o meu livro  ||  ACTUAL ENGLISH --> i found my book  ||  PREDICTED ENGLISH --> i found my book\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> tom é insolente  ||  ACTUAL ENGLISH --> tom is insolent  ||  PREDICTED ENGLISH --> tom is is diet\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "PORTUGUESE --> eu tenho trinta anos  ||  ACTUAL ENGLISH --> i'm thirty  ||  PREDICTED ENGLISH --> i'm bluffing\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.562703\n",
            "Second BLEU score --> 0.414471\n",
            "Third BLEU score --> 0.381239\n",
            "Fourth BLEU score --> 0.295413\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}