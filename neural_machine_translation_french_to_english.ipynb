{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "papermill": {
      "duration": 978.415403,
      "end_time": "2021-01-21T13:49:38.569102",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-21T13:33:20.153699",
      "version": "2.1.0"
    },
    "colab": {
      "name": "neural_machine_translation-french to english.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzRcpO04oCW"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:27.576443Z",
          "iopub.status.busy": "2021-01-21T13:33:27.575716Z",
          "iopub.status.idle": "2021-01-21T13:33:33.308759Z",
          "shell.execute_reply": "2021-01-21T13:33:33.307768Z"
        },
        "papermill": {
          "duration": 5.749362,
          "end_time": "2021-01-21T13:33:33.308907",
          "exception": false,
          "start_time": "2021-01-21T13:33:27.559545",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv76NoqX4oCX",
        "outputId": "a9a66d00-e796-4e19-e3ef-9d58dfda2a45"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "smoothing = SmoothingFunction().method4\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip ./fra-eng.zip\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-09 00:47:11--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 172.67.173.198, 104.21.55.222, 2606:4700:3036::ac43:adc6, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|172.67.173.198|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6413399 (6.1M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   6.12M  5.24MB/s    in 1.2s    \n",
            "\n",
            "2021-06-09 00:47:13 (5.24 MB/s) - ‘fra-eng.zip’ saved [6413399/6413399]\n",
            "\n",
            "Archive:  ./fra-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj6Kqp_k4oCX"
      },
      "source": [
        "### Data cleaning and train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.345280Z",
          "iopub.status.busy": "2021-01-21T13:33:33.343279Z",
          "iopub.status.idle": "2021-01-21T13:33:33.621266Z",
          "shell.execute_reply": "2021-01-21T13:33:33.620256Z"
        },
        "papermill": {
          "duration": 0.301099,
          "end_time": "2021-01-21T13:33:33.621391",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.320292",
          "status": "completed"
        },
        "tags": [],
        "id": "LaapE2Ud4oCY"
      },
      "source": [
        "data_path = '/content/fra.txt' \n",
        "num_sentences = 20000 \n",
        "\n",
        "# opening the text file and getting the data \n",
        "with open(data_path,'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "#count the number of sentences\n",
        "c=0 \n",
        "\n",
        "# data cleaning\n",
        "source_texts,target_texts = [],[]\n",
        "for line in lines: # going through each lines\n",
        "    if c == num_sentences: # if we have 20000 sentences than we will get out of this loop\n",
        "        break \n",
        "    elif '\\t' in line:\n",
        "        op_data,ip_data,_ = line.lower().rstrip().split('\\t') # lowering the data and then spliting the data\n",
        "        # to remove the punctuation we did not include last character\n",
        "        source_text = ip_data[:-1].strip()\n",
        "        target_text = op_data[:-1].strip()\n",
        "        # removing the unprintable character\n",
        "        # for english and french we will take anly alphabets of brespective languages and numbers\n",
        "        target_text = re.sub(\"[^a-z 1-9\\'-]\",\"\",target_text) \n",
        "        source_text = re.sub(\"[^a-zàâãçéèêëîïôœùûüÿ 1-9\\'-]\",\"\",source_text) \n",
        "        source_texts.append(source_text)\n",
        "        target_texts.append(target_text)\n",
        "        c+=1\n",
        "\n",
        "# train_test_split of the source and target data\n",
        "source_train,source_test,target_train,target_test = train_test_split(source_texts,target_texts,test_size = 0.2, random_state= 0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPvULJAh4oCY"
      },
      "source": [
        "### Making the required functions for the data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.652187Z",
          "iopub.status.busy": "2021-01-21T13:33:33.650331Z",
          "iopub.status.idle": "2021-01-21T13:33:33.652845Z",
          "shell.execute_reply": "2021-01-21T13:33:33.653254Z"
        },
        "papermill": {
          "duration": 0.021365,
          "end_time": "2021-01-21T13:33:33.653354",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.631989",
          "status": "completed"
        },
        "tags": [],
        "id": "YjFYZtBD4oCZ"
      },
      "source": [
        "# tokenizer for data\n",
        "def create_tokenizer(texts):\n",
        "    tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "# one_hot encoding of the target data\n",
        "def one_hot(pad_seq,max_sent_length,num_vocab):\n",
        "    target_data_one_hot = np.zeros((len(pad_seq),max_sent_length,num_vocab))\n",
        "    for i,w in enumerate(pad_seq):\n",
        "        for j,d in enumerate(w):\n",
        "            target_data_one_hot[i,j,d] = 1\n",
        "    return target_data_one_hot\n",
        "\n",
        "# for padding the data\n",
        "def encoding_text(tokenizer,text,max_length):\n",
        "    text_seq = tokenizer.texts_to_sequences(text)\n",
        "    pad_seq = pad_sequences(text_seq,maxlen= max_length)\n",
        "    return pad_seq\n",
        "\n",
        "# to find the maximum length of the sentence from data\n",
        "def max_length(text):\n",
        "    return max(len(l.split()) for l in text)\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsB6Npw64oCZ"
      },
      "source": [
        "### Preparing training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxAtfl57FZR",
        "outputId": "26045cfb-fcbd-4226-e135-f7dbcd004a56"
      },
      "source": [
        "# preparing source tokenizer and getting relevant information\n",
        "source_tokenizer = create_tokenizer(source_train)\n",
        "source_vocab = source_tokenizer.word_index\n",
        "num_source_vocab = len(source_vocab)+1\n",
        "max_source_length = max_length(source_train)\n",
        "print(\"Number of Source Vocabulary :\",num_source_vocab)\n",
        "print(\"Maximum Source Length :\",max_source_length)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Source Vocabulary : 6052\n",
            "Maximum Source Length : 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLrtEWly7FW4",
        "outputId": "307f2bcc-1b2c-4ba0-d3e2-31df4005cf62"
      },
      "source": [
        "# preparing target tokenizer and getting relevant information\n",
        "target_tokenizer = create_tokenizer(target_train)\n",
        "target_vocab = target_tokenizer.word_index\n",
        "num_target_vocab = len(target_vocab)+1\n",
        "max_target_length = max_length(target_train)\n",
        "print(\"Number of Target Vocabulary :\",num_target_vocab)\n",
        "print(\"Maximum Target Length :\",max_target_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Target Vocabulary : 3201\n",
            "Maximum Target Length : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtEbpJMf7FUb",
        "outputId": "a652f876-a001-49ed-a361-6fd3b14f2c7a"
      },
      "source": [
        "# preparing the training data\n",
        "# padding of the source sentences\n",
        "source_train_seq_pad = encoding_text(source_tokenizer,source_train,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_train_seq_pad = encoding_text(target_tokenizer,target_train,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_train_seq_pad = one_hot(target_train_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded train source\n",
            "[[   0    0    0 ...    7   77   40]\n",
            " [   0    0    0 ...   50 1537   59]\n",
            " [   0    0    0 ...    0    0 2058]\n",
            " ...\n",
            " [   0    0    0 ...   39   10   95]\n",
            " [   0    0    0 ...   23   34  417]\n",
            " [   0    0    0 ...    0 6051   23]]\n",
            "-------------------------------------\n",
            "Padded train target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.696956Z",
          "iopub.status.busy": "2021-01-21T13:33:33.691743Z",
          "iopub.status.idle": "2021-01-21T13:33:34.882871Z",
          "shell.execute_reply": "2021-01-21T13:33:34.883337Z"
        },
        "papermill": {
          "duration": 1.219756,
          "end_time": "2021-01-21T13:33:34.883461",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.663705",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ih4M5QQ4oCZ",
        "outputId": "c74bef82-9f99-4bbc-fd78-dc689f7e1a9e"
      },
      "source": [
        "# preparing the test data\n",
        "# padding of the source sentences\n",
        "source_test_seq_pad = encoding_text(source_tokenizer,source_test,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_test_seq_pad = encoding_text(target_tokenizer,target_test,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_test_seq_pad = one_hot(target_test_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded test source\n",
            "[[   0    0    0 ...    7   77   40]\n",
            " [   0    0    0 ...   50 1537   59]\n",
            " [   0    0    0 ...    0    0 2058]\n",
            " ...\n",
            " [   0    0    0 ...   39   10   95]\n",
            " [   0    0    0 ...   23   34  417]\n",
            " [   0    0    0 ...    0 6051   23]]\n",
            "-------------------------------------\n",
            "Padded test target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMJ0uDVW4oCa"
      },
      "source": [
        "### Preparing and running the Autoencoder model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:34.914566Z",
          "iopub.status.busy": "2021-01-21T13:33:34.914052Z",
          "iopub.status.idle": "2021-01-21T13:36:55.385009Z",
          "shell.execute_reply": "2021-01-21T13:36:55.384379Z"
        },
        "papermill": {
          "duration": 200.490724,
          "end_time": "2021-01-21T13:36:55.385175",
          "exception": false,
          "start_time": "2021-01-21T13:33:34.894451",
          "status": "completed"
        },
        "scrolled": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bLmntXQ4oCa",
        "outputId": "aee0fb2b-bbf2-4f21-8050-f2f96f1966d0"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(max_source_length,)))\n",
        "model.add(Embedding(num_source_vocab,512,mask_zero=True))\n",
        "model.add(LSTM(512,return_sequences = False))\n",
        "model.add(RepeatVector(max_target_length))\n",
        "model.add(LSTM(512,return_sequences = True))\n",
        "model.add(TimeDistributed(Dense(num_target_vocab,activation = 'softmax')))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 11, 512)           3098624   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 5, 512)            2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 5, 3201)           1642113   \n",
            "=================================================================\n",
            "Total params: 8,939,137\n",
            "Trainable params: 8,939,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LkVZFgi8vLa"
      },
      "source": [
        "es = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01)\n",
        "filepath = './french-english.h5' \n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4PbSust84ba",
        "outputId": "1a8f00c3-53c4-4a03-dca2-ca609a873a27"
      },
      "source": [
        "history = model.fit(source_train_seq_pad, target_train_seq_pad, epochs= 100, batch_size=64, validation_data = (source_test_seq_pad,target_test_seq_pad), verbose=1,callbacks=[checkpoint,es])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 14s 37ms/step - loss: 3.8147 - acc: 0.4193 - val_loss: 3.5579 - val_acc: 0.4261\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.42610, saving model to ./french-english.h5\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 3.2409 - acc: 0.4593 - val_loss: 2.9914 - val_acc: 0.4927\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.42610 to 0.49275, saving model to ./french-english.h5\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 2.7851 - acc: 0.5146 - val_loss: 2.7181 - val_acc: 0.5295\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.49275 to 0.52950, saving model to ./french-english.h5\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 2.4974 - acc: 0.5518 - val_loss: 2.5308 - val_acc: 0.5585\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.52950 to 0.55850, saving model to ./french-english.h5\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 2.2661 - acc: 0.5821 - val_loss: 2.3801 - val_acc: 0.5795\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.55850 to 0.57945, saving model to ./french-english.h5\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 2.0642 - acc: 0.6108 - val_loss: 2.2761 - val_acc: 0.5958\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.57945 to 0.59580, saving model to ./french-english.h5\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 1.8845 - acc: 0.6396 - val_loss: 2.1890 - val_acc: 0.6084\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.59580 to 0.60845, saving model to ./french-english.h5\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 1.7211 - acc: 0.6642 - val_loss: 2.0879 - val_acc: 0.6285\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.60845 to 0.62855, saving model to ./french-english.h5\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 1.5703 - acc: 0.6881 - val_loss: 2.0330 - val_acc: 0.6373\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.62855 to 0.63730, saving model to ./french-english.h5\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 1.4281 - acc: 0.7144 - val_loss: 1.9882 - val_acc: 0.6455\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.63730 to 0.64550, saving model to ./french-english.h5\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 1.3000 - acc: 0.7376 - val_loss: 1.9597 - val_acc: 0.6614\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.64550 to 0.66135, saving model to ./french-english.h5\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 1.1819 - acc: 0.7587 - val_loss: 1.9131 - val_acc: 0.6669\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.66135 to 0.66690, saving model to ./french-english.h5\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 1.0747 - acc: 0.7800 - val_loss: 1.8852 - val_acc: 0.6721\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.66690 to 0.67210, saving model to ./french-english.h5\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.9769 - acc: 0.7990 - val_loss: 1.8718 - val_acc: 0.6787\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.67210 to 0.67865, saving model to ./french-english.h5\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 0.8832 - acc: 0.8177 - val_loss: 1.8707 - val_acc: 0.6865\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.67865 to 0.68645, saving model to ./french-english.h5\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.7990 - acc: 0.8335 - val_loss: 1.8570 - val_acc: 0.6905\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.68645 to 0.69050, saving model to ./french-english.h5\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.7204 - acc: 0.8500 - val_loss: 1.8532 - val_acc: 0.6959\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.69050 to 0.69585, saving model to ./french-english.h5\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.6584 - acc: 0.8625 - val_loss: 1.8519 - val_acc: 0.6959\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.69585 to 0.69595, saving model to ./french-english.h5\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.6003 - acc: 0.8735 - val_loss: 1.8759 - val_acc: 0.6971\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.69595 to 0.69710, saving model to ./french-english.h5\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.5535 - acc: 0.8850 - val_loss: 1.8699 - val_acc: 0.7006\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.69710 to 0.70060, saving model to ./french-english.h5\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.5099 - acc: 0.8949 - val_loss: 1.8621 - val_acc: 0.7064\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.70060 to 0.70640, saving model to ./french-english.h5\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.4748 - acc: 0.9005 - val_loss: 1.8748 - val_acc: 0.7056\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.70640\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 8s 33ms/step - loss: 0.4394 - acc: 0.9064 - val_loss: 1.8967 - val_acc: 0.7094\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.70640 to 0.70940, saving model to ./french-english.h5\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 0.4041 - acc: 0.9124 - val_loss: 1.9069 - val_acc: 0.7085\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.70940\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.3808 - acc: 0.9169 - val_loss: 1.9023 - val_acc: 0.7092\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.70940\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 8s 31ms/step - loss: 0.3552 - acc: 0.9204 - val_loss: 1.9015 - val_acc: 0.7100\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.70940 to 0.71005, saving model to ./french-english.h5\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 0.3361 - acc: 0.9234 - val_loss: 1.9067 - val_acc: 0.7096\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.71005\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 8s 32ms/step - loss: 0.3155 - acc: 0.9264 - val_loss: 1.9281 - val_acc: 0.7096\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.71005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:57.032153Z",
          "iopub.status.busy": "2021-01-21T13:36:57.031063Z",
          "iopub.status.idle": "2021-01-21T13:36:57.047282Z",
          "shell.execute_reply": "2021-01-21T13:36:57.046676Z"
        },
        "papermill": {
          "duration": 0.844983,
          "end_time": "2021-01-21T13:36:57.047395",
          "exception": false,
          "start_time": "2021-01-21T13:36:56.202412",
          "status": "completed"
        },
        "tags": [],
        "id": "jiNVZlvP4oCb"
      },
      "source": [
        "# loading the weights from the best saved model\n",
        "model.load_weights(filepath)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFknwSI64oCb"
      },
      "source": [
        "### Making the functions to predict the sequence and BLEU_sccore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:58.701542Z",
          "iopub.status.busy": "2021-01-21T13:36:58.699715Z",
          "iopub.status.idle": "2021-01-21T13:36:58.702231Z",
          "shell.execute_reply": "2021-01-21T13:36:58.702636Z"
        },
        "papermill": {
          "duration": 0.838898,
          "end_time": "2021-01-21T13:36:58.702743",
          "exception": false,
          "start_time": "2021-01-21T13:36:57.863845",
          "status": "completed"
        },
        "tags": [],
        "id": "N6QTt-LU4oCb"
      },
      "source": [
        "# a dictionary having key is a token number for a particular word and value is a word\n",
        "# this will required to decode the predicted sequence\n",
        "target_vocab_idx = {v:k for k,v in target_tokenizer.word_index.items()}\n",
        "\n",
        "# function to predict the decoded sequence\n",
        "def predict_sequence(model,sent,vocab_idx):\n",
        "    prediction = model.predict(sent.reshape(1,max_source_length))[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = []\n",
        "    for i in integers:\n",
        "        if i != 0:\n",
        "            word = vocab_idx[i]\n",
        "            if word is None:\n",
        "                break\n",
        "            target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "# for evaluation of the model through BLEU_score\n",
        "def bleu_score(model,ip,ip_raw,op_raw,vocab_idx):\n",
        "    prediction,actual = [],[]\n",
        "    for i,sent in enumerate(ip):\n",
        "        if i%10 == 0:\n",
        "            print('\\rprogress ',(i+1)*100//len(ip),'%',sep='',end='',flush = True)\n",
        "        translation = predict_sequence(model,sent,vocab_idx)\n",
        "        prediction.append(translation)\n",
        "        actual.append(op_raw[i])\n",
        "    print()\n",
        "    # printing the first ten sentences\n",
        "    for i in range(10):\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "        print('FRENCH -->',ip_raw[i],' || ','ACTUAL ENGLISH -->',op_raw[i],' || ','PREDICTED ENGLISH -->',prediction[i])\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print()\n",
        "    # printing the BLEU_score\n",
        "    print(\"----------------------------------\")\n",
        "    print('Printing BLEU SCORE...')\n",
        "    print(\"----------------------------------\")\n",
        "    print('First BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(1.0, 0, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Second BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Third BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Fourth BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print(\"----------------------------------\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPcAEs24oCc"
      },
      "source": [
        "### Evaluating the model on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:37:00.637318Z",
          "iopub.status.busy": "2021-01-21T13:37:00.636492Z",
          "iopub.status.idle": "2021-01-21T13:47:02.656690Z",
          "shell.execute_reply": "2021-01-21T13:47:02.656212Z"
        },
        "papermill": {
          "duration": 603.117053,
          "end_time": "2021-01-21T13:47:02.656787",
          "exception": false,
          "start_time": "2021-01-21T13:36:59.539734",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2KUCpGO4oCc",
        "outputId": "96fe70e2-3f2e-441c-a965-7407adc613a4"
      },
      "source": [
        "bleu_score(model,source_train_seq_pad,source_train,target_train,target_vocab_idx)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> je ne vois pas très bien  ||  ACTUAL ENGLISH --> i can't see well  ||  PREDICTED ENGLISH --> i can't see well\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> je fais rarement cela  ||  ACTUAL ENGLISH --> i seldom do that  ||  PREDICTED ENGLISH --> i don't do that\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> calmos  ||  ACTUAL ENGLISH --> slow down  ||  PREDICTED ENGLISH --> slow down\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> attendons ici  ||  ACTUAL ENGLISH --> let's wait here  ||  PREDICTED ENGLISH --> let's wait here\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> j'étais en train de chanter  ||  ACTUAL ENGLISH --> i was singing  ||  PREDICTED ENGLISH --> i was singing\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> c'est la criminalité qui est en baisse  ||  ACTUAL ENGLISH --> crime is down  ||  PREDICTED ENGLISH --> crime is down\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> elle a beaucoup parlé  ||  ACTUAL ENGLISH --> she talked a lot  ||  PREDICTED ENGLISH --> she talked a lot\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> c'est tout ce que j'ai fait  ||  ACTUAL ENGLISH --> that's all i did  ||  PREDICTED ENGLISH --> that's all i did\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> vous n'êtes pas mort  ||  ACTUAL ENGLISH --> you're not dead  ||  PREDICTED ENGLISH --> you're not dead\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> on arrive  ||  ACTUAL ENGLISH --> we're coming  ||  PREDICTED ENGLISH --> we're coming\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.699600\n",
            "Second BLEU score --> 0.476196\n",
            "Third BLEU score --> 0.420058\n",
            "Fourth BLEU score --> 0.323232\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYxGyiZ4oCd"
      },
      "source": [
        "### Evaluating the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:47:05.406595Z",
          "iopub.status.busy": "2021-01-21T13:47:05.405722Z",
          "iopub.status.idle": "2021-01-21T13:49:35.405960Z",
          "shell.execute_reply": "2021-01-21T13:49:35.405286Z"
        },
        "papermill": {
          "duration": 151.514089,
          "end_time": "2021-01-21T13:49:35.406089",
          "exception": false,
          "start_time": "2021-01-21T13:47:03.892000",
          "status": "completed"
        },
        "tags": [],
        "id": "oqw6Q6Fb4oCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e7eae41-97a9-4de9-8b42-e9c60dacd15a"
      },
      "source": [
        "bleu_score(model,source_test_seq_pad,source_test,target_test,target_vocab_idx)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> laissez ça tranquille  ||  ACTUAL ENGLISH --> leave that alone  ||  PREDICTED ENGLISH --> leave that alone\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> j'irai te prendre  ||  ACTUAL ENGLISH --> i'll get you  ||  PREDICTED ENGLISH --> i'll get you\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> vous temporisez  ||  ACTUAL ENGLISH --> you're stalling  ||  PREDICTED ENGLISH --> you crying\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> continue  ||  ACTUAL ENGLISH --> just keep moving  ||  PREDICTED ENGLISH --> on with it\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> c'est compliqué  ||  ACTUAL ENGLISH --> it's complex  ||  PREDICTED ENGLISH --> it's tom\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> décampez  ||  ACTUAL ENGLISH --> get out  ||  PREDICTED ENGLISH --> get lost\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> je sens le gaz  ||  ACTUAL ENGLISH --> i smell gas  ||  PREDICTED ENGLISH --> i'm the tv\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> ils sont cassés  ||  ACTUAL ENGLISH --> they're broken  ||  PREDICTED ENGLISH --> they're left\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> j'ai entendu un bruit  ||  ACTUAL ENGLISH --> i heard a noise  ||  PREDICTED ENGLISH --> i heard a heard\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "FRENCH --> sois prudent  ||  ACTUAL ENGLISH --> be careful  ||  PREDICTED ENGLISH --> be careful\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.584117\n",
            "Second BLEU score --> 0.445027\n",
            "Third BLEU score --> 0.407450\n",
            "Fourth BLEU score --> 0.317267\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}