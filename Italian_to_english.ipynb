{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "papermill": {
      "duration": 978.415403,
      "end_time": "2021-01-21T13:49:38.569102",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-21T13:33:20.153699",
      "version": "2.1.0"
    },
    "colab": {
      "name": "Italian to english.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzRcpO04oCW"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:27.576443Z",
          "iopub.status.busy": "2021-01-21T13:33:27.575716Z",
          "iopub.status.idle": "2021-01-21T13:33:33.308759Z",
          "shell.execute_reply": "2021-01-21T13:33:33.307768Z"
        },
        "papermill": {
          "duration": 5.749362,
          "end_time": "2021-01-21T13:33:33.308907",
          "exception": false,
          "start_time": "2021-01-21T13:33:27.559545",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv76NoqX4oCX",
        "outputId": "4301dc61-0173-495e-80fa-293b879dc194"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "smoothing = SmoothingFunction().method4\n",
        "!wget http://www.manythings.org/anki/ita-eng.zip\n",
        "!unzip ./ita-eng.zip\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-09 02:00:06--  http://www.manythings.org/anki/ita-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3031::6815:37de, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7645726 (7.3M) [application/zip]\n",
            "Saving to: ‘ita-eng.zip’\n",
            "\n",
            "ita-eng.zip         100%[===================>]   7.29M  4.89MB/s    in 1.5s    \n",
            "\n",
            "2021-06-09 02:00:08 (4.89 MB/s) - ‘ita-eng.zip’ saved [7645726/7645726]\n",
            "\n",
            "Archive:  ./ita-eng.zip\n",
            "  inflating: ita.txt                 \n",
            "  inflating: _about.txt              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj6Kqp_k4oCX"
      },
      "source": [
        "### Data cleaning and train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.345280Z",
          "iopub.status.busy": "2021-01-21T13:33:33.343279Z",
          "iopub.status.idle": "2021-01-21T13:33:33.621266Z",
          "shell.execute_reply": "2021-01-21T13:33:33.620256Z"
        },
        "papermill": {
          "duration": 0.301099,
          "end_time": "2021-01-21T13:33:33.621391",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.320292",
          "status": "completed"
        },
        "tags": [],
        "id": "LaapE2Ud4oCY"
      },
      "source": [
        "data_path = '/content/ita.txt' \n",
        "num_sentences = 20000 \n",
        "\n",
        "# opening the text file and getting the data \n",
        "with open(data_path,'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "#count the number of sentences\n",
        "c=0 \n",
        "\n",
        "# data cleaning\n",
        "source_texts,target_texts = [],[]\n",
        "for line in lines: # going through each lines\n",
        "    if c == num_sentences: # if we have 20000 sentences than we will get out of this loop\n",
        "        break \n",
        "    elif '\\t' in line:\n",
        "        op_data,ip_data,_ = line.lower().rstrip().split('\\t') # lowering the data and then spliting the data\n",
        "        # to remove the punctuation we did not include last character\n",
        "        source_text = ip_data[:-1].strip()\n",
        "        target_text = op_data[:-1].strip()\n",
        "        # removing the unprintable character\n",
        "        # for english and french we will take anly alphabets of brespective languages and numbers\n",
        "        target_text = re.sub(\"[^a-z 1-9\\'-]\",\"\",target_text) \n",
        "        source_text = re.sub(\"[^a-zàâãçéèêëîïôœùûüÿ 1-9\\'-]\",\"\",source_text) \n",
        "        source_texts.append(source_text)\n",
        "        target_texts.append(target_text)\n",
        "        c+=1\n",
        "\n",
        "# train_test_split of the source and target data\n",
        "source_train,source_test,target_train,target_test = train_test_split(source_texts,target_texts,test_size = 0.2, random_state= 0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPvULJAh4oCY"
      },
      "source": [
        "### Making the required functions for the data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.652187Z",
          "iopub.status.busy": "2021-01-21T13:33:33.650331Z",
          "iopub.status.idle": "2021-01-21T13:33:33.652845Z",
          "shell.execute_reply": "2021-01-21T13:33:33.653254Z"
        },
        "papermill": {
          "duration": 0.021365,
          "end_time": "2021-01-21T13:33:33.653354",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.631989",
          "status": "completed"
        },
        "tags": [],
        "id": "YjFYZtBD4oCZ"
      },
      "source": [
        "# tokenizer for data\n",
        "def create_tokenizer(texts):\n",
        "    tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "# one_hot encoding of the target data\n",
        "def one_hot(pad_seq,max_sent_length,num_vocab):\n",
        "    target_data_one_hot = np.zeros((len(pad_seq),max_sent_length,num_vocab))\n",
        "    for i,w in enumerate(pad_seq):\n",
        "        for j,d in enumerate(w):\n",
        "            target_data_one_hot[i,j,d] = 1\n",
        "    return target_data_one_hot\n",
        "\n",
        "# for padding the data\n",
        "def encoding_text(tokenizer,text,max_length):\n",
        "    text_seq = tokenizer.texts_to_sequences(text)\n",
        "    pad_seq = pad_sequences(text_seq,maxlen= max_length)\n",
        "    return pad_seq\n",
        "\n",
        "# to find the maximum length of the sentence from data\n",
        "def max_length(text):\n",
        "    return max(len(l.split()) for l in text)\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsB6Npw64oCZ"
      },
      "source": [
        "### Preparing training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxAtfl57FZR",
        "outputId": "cc277412-e5fa-4871-c794-48e3e5c77641"
      },
      "source": [
        "# preparing source tokenizer and getting relevant information\n",
        "source_tokenizer = create_tokenizer(source_train)\n",
        "source_vocab = source_tokenizer.word_index\n",
        "num_source_vocab = len(source_vocab)+1\n",
        "max_source_length = max_length(source_train)\n",
        "print(\"Number of Source Vocabulary :\",num_source_vocab)\n",
        "print(\"Maximum Source Length :\",max_source_length)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Source Vocabulary : 5271\n",
            "Maximum Source Length : 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLrtEWly7FW4",
        "outputId": "ab677427-1a91-4d75-a385-0967e7d0854f"
      },
      "source": [
        "# preparing target tokenizer and getting relevant information\n",
        "target_tokenizer = create_tokenizer(target_train)\n",
        "target_vocab = target_tokenizer.word_index\n",
        "num_target_vocab = len(target_vocab)+1\n",
        "max_target_length = max_length(target_train)\n",
        "print(\"Number of Target Vocabulary :\",num_target_vocab)\n",
        "print(\"Maximum Target Length :\",max_target_length)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Target Vocabulary : 2224\n",
            "Maximum Target Length : 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtEbpJMf7FUb",
        "outputId": "526e6773-a594-4d9a-9c2e-d5e2e90f5616"
      },
      "source": [
        "# preparing the training data\n",
        "# padding of the source sentences\n",
        "source_train_seq_pad = encoding_text(source_tokenizer,source_train,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_train_seq_pad = encoding_text(target_tokenizer,target_train,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_train_seq_pad = one_hot(target_train_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded train source\n",
            "[[   0    0    0 ...    0   57   39]\n",
            " [   0    0    0 ...    5   11 2006]\n",
            " [   0    0    0 ...    0 2007    4]\n",
            " ...\n",
            " [   0    0   25 ...   37   12   27]\n",
            " [   0    0    0 ...   14   16  978]\n",
            " [   0    0    0 ...    0    2  101]]\n",
            "-------------------------------------\n",
            "Padded train target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.696956Z",
          "iopub.status.busy": "2021-01-21T13:33:33.691743Z",
          "iopub.status.idle": "2021-01-21T13:33:34.882871Z",
          "shell.execute_reply": "2021-01-21T13:33:34.883337Z"
        },
        "papermill": {
          "duration": 1.219756,
          "end_time": "2021-01-21T13:33:34.883461",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.663705",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ih4M5QQ4oCZ",
        "outputId": "87df5840-ae1a-4806-e38f-b283c89285ed"
      },
      "source": [
        "# preparing the test data\n",
        "# padding of the source sentences\n",
        "source_test_seq_pad = encoding_text(source_tokenizer,source_test,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_test_seq_pad = encoding_text(target_tokenizer,target_test,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_test_seq_pad = one_hot(target_test_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded test source\n",
            "[[   0    0    0 ...    0   57   39]\n",
            " [   0    0    0 ...    5   11 2006]\n",
            " [   0    0    0 ...    0 2007    4]\n",
            " ...\n",
            " [   0    0   25 ...   37   12   27]\n",
            " [   0    0    0 ...   14   16  978]\n",
            " [   0    0    0 ...    0    2  101]]\n",
            "-------------------------------------\n",
            "Padded test target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 1. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMJ0uDVW4oCa"
      },
      "source": [
        "### Preparing and running the Autoencoder model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:34.914566Z",
          "iopub.status.busy": "2021-01-21T13:33:34.914052Z",
          "iopub.status.idle": "2021-01-21T13:36:55.385009Z",
          "shell.execute_reply": "2021-01-21T13:36:55.384379Z"
        },
        "papermill": {
          "duration": 200.490724,
          "end_time": "2021-01-21T13:36:55.385175",
          "exception": false,
          "start_time": "2021-01-21T13:33:34.894451",
          "status": "completed"
        },
        "scrolled": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bLmntXQ4oCa",
        "outputId": "a98c5958-b619-4fd8-f253-eec133b3c508"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(max_source_length,)))\n",
        "model.add(Embedding(num_source_vocab,512,mask_zero=True))\n",
        "model.add(LSTM(512,return_sequences = False))\n",
        "model.add(RepeatVector(max_target_length))\n",
        "model.add(LSTM(512,return_sequences = True))\n",
        "model.add(TimeDistributed(Dense(num_target_vocab,activation = 'softmax')))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 7, 512)            2698752   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 5, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 5, 512)            2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 5, 2224)           1140912   \n",
            "=================================================================\n",
            "Total params: 8,038,064\n",
            "Trainable params: 8,038,064\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LkVZFgi8vLa"
      },
      "source": [
        "es = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01)\n",
        "filepath = './Italian-english.h5' \n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4PbSust84ba",
        "outputId": "b4722c95-eef0-4aee-bc5e-ac1dc83fcc13"
      },
      "source": [
        "history = model.fit(source_train_seq_pad, target_train_seq_pad, epochs= 100, batch_size=64, validation_data = (source_test_seq_pad,target_test_seq_pad), verbose=1,callbacks=[checkpoint,es])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 13s 32ms/step - loss: 3.2037 - acc: 0.4971 - val_loss: 2.7514 - val_acc: 0.5242\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.52420, saving model to ./Italian-english.h5\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.4773 - acc: 0.5580 - val_loss: 2.3423 - val_acc: 0.5776\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.52420 to 0.57760, saving model to ./Italian-english.h5\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 2.1212 - acc: 0.6055 - val_loss: 2.0915 - val_acc: 0.6137\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.57760 to 0.61370, saving model to ./Italian-english.h5\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.8174 - acc: 0.6461 - val_loss: 1.8505 - val_acc: 0.6497\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.61370 to 0.64975, saving model to ./Italian-english.h5\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.5553 - acc: 0.6850 - val_loss: 1.6647 - val_acc: 0.6814\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.64975 to 0.68140, saving model to ./Italian-english.h5\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.3332 - acc: 0.7215 - val_loss: 1.5234 - val_acc: 0.7014\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.68140 to 0.70140, saving model to ./Italian-english.h5\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 1.1492 - acc: 0.7562 - val_loss: 1.4097 - val_acc: 0.7266\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.70140 to 0.72665, saving model to ./Italian-english.h5\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.9946 - acc: 0.7867 - val_loss: 1.3117 - val_acc: 0.7451\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.72665 to 0.74510, saving model to ./Italian-english.h5\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.8585 - acc: 0.8131 - val_loss: 1.2407 - val_acc: 0.7614\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.74510 to 0.76140, saving model to ./Italian-english.h5\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.7425 - acc: 0.8376 - val_loss: 1.1709 - val_acc: 0.7757\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.76140 to 0.77565, saving model to ./Italian-english.h5\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.6491 - acc: 0.8584 - val_loss: 1.1241 - val_acc: 0.7871\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.77565 to 0.78710, saving model to ./Italian-english.h5\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.5670 - acc: 0.8739 - val_loss: 1.0769 - val_acc: 0.8002\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.78710 to 0.80025, saving model to ./Italian-english.h5\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.5004 - acc: 0.8894 - val_loss: 1.0549 - val_acc: 0.8053\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.80025 to 0.80530, saving model to ./Italian-english.h5\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.4385 - acc: 0.9002 - val_loss: 1.0530 - val_acc: 0.8111\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.80530 to 0.81110, saving model to ./Italian-english.h5\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.3825 - acc: 0.9111 - val_loss: 0.9945 - val_acc: 0.8166\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.81110 to 0.81660, saving model to ./Italian-english.h5\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.3433 - acc: 0.9181 - val_loss: 0.9742 - val_acc: 0.8264\n",
            "\n",
            "Epoch 00016: val_acc improved from 0.81660 to 0.82645, saving model to ./Italian-english.h5\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.3083 - acc: 0.9230 - val_loss: 0.9500 - val_acc: 0.8288\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.82645 to 0.82880, saving model to ./Italian-english.h5\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.2842 - acc: 0.9284 - val_loss: 0.9491 - val_acc: 0.8319\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.82880 to 0.83190, saving model to ./Italian-english.h5\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.2602 - acc: 0.9334 - val_loss: 0.9598 - val_acc: 0.8340\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.83190 to 0.83400, saving model to ./Italian-english.h5\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.2424 - acc: 0.9357 - val_loss: 0.9363 - val_acc: 0.8353\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.83400 to 0.83525, saving model to ./Italian-english.h5\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.2272 - acc: 0.9395 - val_loss: 0.9282 - val_acc: 0.8375\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.83525 to 0.83745, saving model to ./Italian-english.h5\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.2137 - acc: 0.9408 - val_loss: 0.9444 - val_acc: 0.8418\n",
            "\n",
            "Epoch 00022: val_acc improved from 0.83745 to 0.84175, saving model to ./Italian-english.h5\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.2021 - acc: 0.9429 - val_loss: 0.9440 - val_acc: 0.8411\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.84175\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.1924 - acc: 0.9448 - val_loss: 0.9466 - val_acc: 0.8406\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.84175\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.1831 - acc: 0.9463 - val_loss: 0.9418 - val_acc: 0.8406\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.84175\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 6s 24ms/step - loss: 0.1773 - acc: 0.9473 - val_loss: 0.9321 - val_acc: 0.8441\n",
            "\n",
            "Epoch 00026: val_acc improved from 0.84175 to 0.84410, saving model to ./Italian-english.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:57.032153Z",
          "iopub.status.busy": "2021-01-21T13:36:57.031063Z",
          "iopub.status.idle": "2021-01-21T13:36:57.047282Z",
          "shell.execute_reply": "2021-01-21T13:36:57.046676Z"
        },
        "papermill": {
          "duration": 0.844983,
          "end_time": "2021-01-21T13:36:57.047395",
          "exception": false,
          "start_time": "2021-01-21T13:36:56.202412",
          "status": "completed"
        },
        "tags": [],
        "id": "jiNVZlvP4oCb"
      },
      "source": [
        "# loading the weights from the best saved model\n",
        "model.load_weights(filepath)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFknwSI64oCb"
      },
      "source": [
        "### Making the functions to predict the sequence and BLEU_sccore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:58.701542Z",
          "iopub.status.busy": "2021-01-21T13:36:58.699715Z",
          "iopub.status.idle": "2021-01-21T13:36:58.702231Z",
          "shell.execute_reply": "2021-01-21T13:36:58.702636Z"
        },
        "papermill": {
          "duration": 0.838898,
          "end_time": "2021-01-21T13:36:58.702743",
          "exception": false,
          "start_time": "2021-01-21T13:36:57.863845",
          "status": "completed"
        },
        "tags": [],
        "id": "N6QTt-LU4oCb"
      },
      "source": [
        "# a dictionary having key is a token number for a particular word and value is a word\n",
        "# this will required to decode the predicted sequence\n",
        "target_vocab_idx = {v:k for k,v in target_tokenizer.word_index.items()}\n",
        "\n",
        "# function to predict the decoded sequence\n",
        "def predict_sequence(model,sent,vocab_idx):\n",
        "    prediction = model.predict(sent.reshape(1,max_source_length))[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = []\n",
        "    for i in integers:\n",
        "        if i != 0:\n",
        "            word = vocab_idx[i]\n",
        "            if word is None:\n",
        "                break\n",
        "            target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "# for evaluation of the model through BLEU_score\n",
        "def bleu_score(model,ip,ip_raw,op_raw,vocab_idx):\n",
        "    prediction,actual = [],[]\n",
        "    for i,sent in enumerate(ip):\n",
        "        if i%10 == 0:\n",
        "            print('\\rprogress ',(i+1)*100//len(ip),'%',sep='',end='',flush = True)\n",
        "        translation = predict_sequence(model,sent,vocab_idx)\n",
        "        prediction.append(translation)\n",
        "        actual.append(op_raw[i])\n",
        "    print()\n",
        "    # printing the first ten sentences\n",
        "    for i in range(10):\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "        print('ITALIAN -->',ip_raw[i],' || ','ACTUAL ENGLISH -->',op_raw[i],' || ','PREDICTED ENGLISH -->',prediction[i])\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print()\n",
        "    # printing the BLEU_score\n",
        "    print(\"----------------------------------\")\n",
        "    print('Printing BLEU SCORE...')\n",
        "    print(\"----------------------------------\")\n",
        "    print('First BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(1.0, 0, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Second BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Third BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Fourth BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print(\"----------------------------------\")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPcAEs24oCc"
      },
      "source": [
        "### Evaluating the model on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:37:00.637318Z",
          "iopub.status.busy": "2021-01-21T13:37:00.636492Z",
          "iopub.status.idle": "2021-01-21T13:47:02.656690Z",
          "shell.execute_reply": "2021-01-21T13:47:02.656212Z"
        },
        "papermill": {
          "duration": 603.117053,
          "end_time": "2021-01-21T13:47:02.656787",
          "exception": false,
          "start_time": "2021-01-21T13:36:59.539734",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2KUCpGO4oCc",
        "outputId": "eb811721-6755-4e1c-da13-7d5dca6c1ebe"
      },
      "source": [
        "bleu_score(model,source_train_seq_pad,source_train,target_train,target_vocab_idx)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> ti piace  ||  ACTUAL ENGLISH --> do you like it  ||  PREDICTED ENGLISH --> do i like it\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> è un animale  ||  ACTUAL ENGLISH --> he's an animal  ||  PREDICTED ENGLISH --> he's an animal\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> morsi tom  ||  ACTUAL ENGLISH --> i bit tom  ||  PREDICTED ENGLISH --> i bit tom\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> ci vediamo più tardi  ||  ACTUAL ENGLISH --> see you later  ||  PREDICTED ENGLISH --> see you later\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> porta dentro tom  ||  ACTUAL ENGLISH --> bring tom in  ||  PREDICTED ENGLISH --> bring tom in\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> tom la lasci  ||  ACTUAL ENGLISH --> tom left it  ||  PREDICTED ENGLISH --> tom left it\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> stavo pulendo  ||  ACTUAL ENGLISH --> i was cleaning  ||  PREDICTED ENGLISH --> i was it\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> ritorner  ||  ACTUAL ENGLISH --> i'll come back  ||  PREDICTED ENGLISH --> i will return\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> siamo ricche  ||  ACTUAL ENGLISH --> we're wealthy  ||  PREDICTED ENGLISH --> we're wealthy\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> lei è pigra  ||  ACTUAL ENGLISH --> she is lazy  ||  PREDICTED ENGLISH --> you're lazy\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.752738\n",
            "Second BLEU score --> 0.505194\n",
            "Third BLEU score --> 0.439660\n",
            "Fourth BLEU score --> 0.338034\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYxGyiZ4oCd"
      },
      "source": [
        "### Evaluating the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:47:05.406595Z",
          "iopub.status.busy": "2021-01-21T13:47:05.405722Z",
          "iopub.status.idle": "2021-01-21T13:49:35.405960Z",
          "shell.execute_reply": "2021-01-21T13:49:35.405286Z"
        },
        "papermill": {
          "duration": 151.514089,
          "end_time": "2021-01-21T13:49:35.406089",
          "exception": false,
          "start_time": "2021-01-21T13:47:03.892000",
          "status": "completed"
        },
        "tags": [],
        "id": "oqw6Q6Fb4oCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757216af-0a5a-43f5-acd2-aea758271c02"
      },
      "source": [
        "bleu_score(model,source_test_seq_pad,source_test,target_test,target_vocab_idx)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> amo i partiti  ||  ACTUAL ENGLISH --> i love parties  ||  PREDICTED ENGLISH --> i love\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> ho sentito tom  ||  ACTUAL ENGLISH --> i heard tom  ||  PREDICTED ENGLISH --> i heard tom\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> che seccatura  ||  ACTUAL ENGLISH --> what a hassle  ||  PREDICTED ENGLISH --> what a night\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> adoro qui  ||  ACTUAL ENGLISH --> i love it here  ||  PREDICTED ENGLISH --> i love it here\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> ero terzo  ||  ACTUAL ENGLISH --> i was third  ||  PREDICTED ENGLISH --> i was ecstatic\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> entri  ||  ACTUAL ENGLISH --> come in  ||  PREDICTED ENGLISH --> come in\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> io ho paura  ||  ACTUAL ENGLISH --> i'm afraid  ||  PREDICTED ENGLISH --> i'm afraid\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> siete coraggiosi  ||  ACTUAL ENGLISH --> are you brave  ||  PREDICTED ENGLISH --> you're brave\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> sono impegnata tom  ||  ACTUAL ENGLISH --> i'm busy tom  ||  PREDICTED ENGLISH --> i'm busy tom\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "ITALIAN --> fallir  ||  ACTUAL ENGLISH --> i'll fail  ||  PREDICTED ENGLISH --> i'll fail\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.677158\n",
            "Second BLEU score --> 0.445984\n",
            "Third BLEU score --> 0.394841\n",
            "Fourth BLEU score --> 0.302341\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}