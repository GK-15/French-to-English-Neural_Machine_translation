{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "papermill": {
      "duration": 978.415403,
      "end_time": "2021-01-21T13:49:38.569102",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-01-21T13:33:20.153699",
      "version": "2.1.0"
    },
    "colab": {
      "name": "Spanish to english.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKzRcpO04oCW"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:27.576443Z",
          "iopub.status.busy": "2021-01-21T13:33:27.575716Z",
          "iopub.status.idle": "2021-01-21T13:33:33.308759Z",
          "shell.execute_reply": "2021-01-21T13:33:33.307768Z"
        },
        "papermill": {
          "duration": 5.749362,
          "end_time": "2021-01-21T13:33:33.308907",
          "exception": false,
          "start_time": "2021-01-21T13:33:27.559545",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv76NoqX4oCX",
        "outputId": "1df618cf-f717-4611-c62a-be6cf5f2b3e3"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "smoothing = SmoothingFunction().method4\n",
        "!wget http://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip ./spa-eng.zip\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-09 01:34:23--  http://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 104.21.55.222, 172.67.173.198, 2606:4700:3036::ac43:adc6, ...\n",
            "Connecting to www.manythings.org (www.manythings.org)|104.21.55.222|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5084241 (4.8M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   4.85M  4.64MB/s    in 1.0s    \n",
            "\n",
            "2021-06-09 01:34:25 (4.64 MB/s) - ‘spa-eng.zip’ saved [5084241/5084241]\n",
            "\n",
            "Archive:  ./spa-eng.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: yes\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj6Kqp_k4oCX"
      },
      "source": [
        "### Data cleaning and train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.345280Z",
          "iopub.status.busy": "2021-01-21T13:33:33.343279Z",
          "iopub.status.idle": "2021-01-21T13:33:33.621266Z",
          "shell.execute_reply": "2021-01-21T13:33:33.620256Z"
        },
        "papermill": {
          "duration": 0.301099,
          "end_time": "2021-01-21T13:33:33.621391",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.320292",
          "status": "completed"
        },
        "tags": [],
        "id": "LaapE2Ud4oCY"
      },
      "source": [
        "data_path = '/content/spa.txt' \n",
        "num_sentences = 20000 \n",
        "\n",
        "# opening the text file and getting the data \n",
        "with open(data_path,'r') as f:\n",
        "    lines = f.read().split('\\n')\n",
        "#count the number of sentences\n",
        "c=0 \n",
        "\n",
        "# data cleaning\n",
        "source_texts,target_texts = [],[]\n",
        "for line in lines: # going through each lines\n",
        "    if c == num_sentences: # if we have 20000 sentences than we will get out of this loop\n",
        "        break \n",
        "    elif '\\t' in line:\n",
        "        op_data,ip_data,_ = line.lower().rstrip().split('\\t') # lowering the data and then spliting the data\n",
        "        # to remove the punctuation we did not include last character\n",
        "        source_text = ip_data[:-1].strip()\n",
        "        target_text = op_data[:-1].strip()\n",
        "        # removing the unprintable character\n",
        "        # for english and french we will take anly alphabets of brespective languages and numbers\n",
        "        target_text = re.sub(\"[^a-z 1-9\\'-]\",\"\",target_text) \n",
        "        source_text = re.sub(\"[^a-zàâãçéèêëîïôœùûüÿ 1-9\\'-]\",\"\",source_text) \n",
        "        source_texts.append(source_text)\n",
        "        target_texts.append(target_text)\n",
        "        c+=1\n",
        "\n",
        "# train_test_split of the source and target data\n",
        "source_train,source_test,target_train,target_test = train_test_split(source_texts,target_texts,test_size = 0.2, random_state= 0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPvULJAh4oCY"
      },
      "source": [
        "### Making the required functions for the data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.652187Z",
          "iopub.status.busy": "2021-01-21T13:33:33.650331Z",
          "iopub.status.idle": "2021-01-21T13:33:33.652845Z",
          "shell.execute_reply": "2021-01-21T13:33:33.653254Z"
        },
        "papermill": {
          "duration": 0.021365,
          "end_time": "2021-01-21T13:33:33.653354",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.631989",
          "status": "completed"
        },
        "tags": [],
        "id": "YjFYZtBD4oCZ"
      },
      "source": [
        "# tokenizer for data\n",
        "def create_tokenizer(texts):\n",
        "    tokenizer = Tokenizer(oov_token='<UNK>')\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "# one_hot encoding of the target data\n",
        "def one_hot(pad_seq,max_sent_length,num_vocab):\n",
        "    target_data_one_hot = np.zeros((len(pad_seq),max_sent_length,num_vocab))\n",
        "    for i,w in enumerate(pad_seq):\n",
        "        for j,d in enumerate(w):\n",
        "            target_data_one_hot[i,j,d] = 1\n",
        "    return target_data_one_hot\n",
        "\n",
        "# for padding the data\n",
        "def encoding_text(tokenizer,text,max_length):\n",
        "    text_seq = tokenizer.texts_to_sequences(text)\n",
        "    pad_seq = pad_sequences(text_seq,maxlen= max_length)\n",
        "    return pad_seq\n",
        "\n",
        "# to find the maximum length of the sentence from data\n",
        "def max_length(text):\n",
        "    return max(len(l.split()) for l in text)\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsB6Npw64oCZ"
      },
      "source": [
        "### Preparing training and testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzxAtfl57FZR",
        "outputId": "e2618238-8684-4a70-db0b-c914332e3afa"
      },
      "source": [
        "# preparing source tokenizer and getting relevant information\n",
        "source_tokenizer = create_tokenizer(source_train)\n",
        "source_vocab = source_tokenizer.word_index\n",
        "num_source_vocab = len(source_vocab)+1\n",
        "max_source_length = max_length(source_train)\n",
        "print(\"Number of Source Vocabulary :\",num_source_vocab)\n",
        "print(\"Maximum Source Length :\",max_source_length)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Source Vocabulary : 6530\n",
            "Maximum Source Length : 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLrtEWly7FW4",
        "outputId": "2f08db3d-de8b-4b0e-cd4a-f948618b56fa"
      },
      "source": [
        "# preparing target tokenizer and getting relevant information\n",
        "target_tokenizer = create_tokenizer(target_train)\n",
        "target_vocab = target_tokenizer.word_index\n",
        "num_target_vocab = len(target_vocab)+1\n",
        "max_target_length = max_length(target_train)\n",
        "print(\"Number of Target Vocabulary :\",num_target_vocab)\n",
        "print(\"Maximum Target Length :\",max_target_length)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Target Vocabulary : 3468\n",
            "Maximum Target Length : 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtEbpJMf7FUb",
        "outputId": "0efc46b9-e743-492f-ba3c-ae8009df1f50"
      },
      "source": [
        "# preparing the training data\n",
        "# padding of the source sentences\n",
        "source_train_seq_pad = encoding_text(source_tokenizer,source_train,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_train_seq_pad = encoding_text(target_tokenizer,target_train,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_train_seq_pad = one_hot(target_train_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded train target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded train source\n",
            "[[   0    0    0 ...  258    3  148]\n",
            " [   0    0    0 ...   33   12  595]\n",
            " [   0    0    0 ...    0   24 1229]\n",
            " ...\n",
            " [   0    0    0 ...   64  127    2]\n",
            " [   0    0    0 ...   64  133 1913]\n",
            " [   0    0    0 ...    0  106 1685]]\n",
            "-------------------------------------\n",
            "Padded train target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:33.696956Z",
          "iopub.status.busy": "2021-01-21T13:33:33.691743Z",
          "iopub.status.idle": "2021-01-21T13:33:34.882871Z",
          "shell.execute_reply": "2021-01-21T13:33:34.883337Z"
        },
        "papermill": {
          "duration": 1.219756,
          "end_time": "2021-01-21T13:33:34.883461",
          "exception": false,
          "start_time": "2021-01-21T13:33:33.663705",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ih4M5QQ4oCZ",
        "outputId": "ffca971a-c87e-4929-d7f8-2858e386d706"
      },
      "source": [
        "# preparing the test data\n",
        "# padding of the source sentences\n",
        "source_test_seq_pad = encoding_text(source_tokenizer,source_test,max_source_length) \n",
        "# padding of the target sentences\n",
        "target_test_seq_pad = encoding_text(target_tokenizer,target_test,max_target_length) \n",
        "# one hot encoding of the padded target senteces\n",
        "target_test_seq_pad = one_hot(target_test_seq_pad,max_target_length,num_target_vocab) \n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test source\")\n",
        "print(source_train_seq_pad)\n",
        "print(\"-------------------------------------\")\n",
        "print(\"Padded test target\")\n",
        "print(target_train_seq_pad)\n",
        "print(\"-------------------------------------\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-------------------------------------\n",
            "Padded test source\n",
            "[[   0    0    0 ...  258    3  148]\n",
            " [   0    0    0 ...   33   12  595]\n",
            " [   0    0    0 ...    0   24 1229]\n",
            " ...\n",
            " [   0    0    0 ...   64  127    2]\n",
            " [   0    0    0 ...   64  133 1913]\n",
            " [   0    0    0 ...    0  106 1685]]\n",
            "-------------------------------------\n",
            "Padded test target\n",
            "[[[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]\n",
            "\n",
            " [[1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [1. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]\n",
            "  [0. 0. 0. ... 0. 0. 0.]]]\n",
            "-------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMJ0uDVW4oCa"
      },
      "source": [
        "### Preparing and running the Autoencoder model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:33:34.914566Z",
          "iopub.status.busy": "2021-01-21T13:33:34.914052Z",
          "iopub.status.idle": "2021-01-21T13:36:55.385009Z",
          "shell.execute_reply": "2021-01-21T13:36:55.384379Z"
        },
        "papermill": {
          "duration": 200.490724,
          "end_time": "2021-01-21T13:36:55.385175",
          "exception": false,
          "start_time": "2021-01-21T13:33:34.894451",
          "status": "completed"
        },
        "scrolled": false,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bLmntXQ4oCa",
        "outputId": "187786d2-d80b-4f59-e96e-2b4cac8a1d0b"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Input(shape=(max_source_length,)))\n",
        "model.add(Embedding(num_source_vocab,512,mask_zero=True))\n",
        "model.add(LSTM(512,return_sequences = False))\n",
        "model.add(RepeatVector(max_target_length))\n",
        "model.add(LSTM(512,return_sequences = True))\n",
        "model.add(TimeDistributed(Dense(num_target_vocab,activation = 'softmax')))\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 15, 512)           3343360   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 6, 512)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 6, 512)            2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 6, 3468)           1779084   \n",
            "=================================================================\n",
            "Total params: 9,320,844\n",
            "Trainable params: 9,320,844\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LkVZFgi8vLa"
      },
      "source": [
        "es = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01)\n",
        "filepath = './spanish-english.h5' \n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') "
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4PbSust84ba",
        "outputId": "9e3df52f-aa74-458c-cc10-0cb2bf13aec2"
      },
      "source": [
        "history = model.fit(source_train_seq_pad, target_train_seq_pad, epochs= 100, batch_size=64, validation_data = (source_test_seq_pad,target_test_seq_pad), verbose=1,callbacks=[checkpoint,es])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "250/250 [==============================] - 14s 41ms/step - loss: 3.5280 - acc: 0.4665 - val_loss: 3.1750 - val_acc: 0.4835\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.48354, saving model to ./spanish-english.h5\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 9s 35ms/step - loss: 2.9636 - acc: 0.4994 - val_loss: 2.9004 - val_acc: 0.5127\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.48354 to 0.51267, saving model to ./spanish-english.h5\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 2.6743 - acc: 0.5319 - val_loss: 2.6605 - val_acc: 0.5378\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.51267 to 0.53783, saving model to ./spanish-english.h5\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 2.4099 - acc: 0.5663 - val_loss: 2.4866 - val_acc: 0.5660\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.53783 to 0.56604, saving model to ./spanish-english.h5\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 2.1729 - acc: 0.5988 - val_loss: 2.3414 - val_acc: 0.5925\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.56604 to 0.59254, saving model to ./spanish-english.h5\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.9676 - acc: 0.6293 - val_loss: 2.2560 - val_acc: 0.6099\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.59254 to 0.60992, saving model to ./spanish-english.h5\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.7967 - acc: 0.6556 - val_loss: 2.1553 - val_acc: 0.6233\n",
            "\n",
            "Epoch 00007: val_acc improved from 0.60992 to 0.62329, saving model to ./spanish-english.h5\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.6374 - acc: 0.6813 - val_loss: 2.1210 - val_acc: 0.6342\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.62329 to 0.63421, saving model to ./spanish-english.h5\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.4863 - acc: 0.7055 - val_loss: 2.0592 - val_acc: 0.6428\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.63421 to 0.64283, saving model to ./spanish-english.h5\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.3464 - acc: 0.7290 - val_loss: 2.0408 - val_acc: 0.6532\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.64283 to 0.65325, saving model to ./spanish-english.h5\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.2305 - acc: 0.7514 - val_loss: 1.9923 - val_acc: 0.6630\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.65325 to 0.66296, saving model to ./spanish-english.h5\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.1226 - acc: 0.7716 - val_loss: 1.9992 - val_acc: 0.6641\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.66296 to 0.66413, saving model to ./spanish-english.h5\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 1.0131 - acc: 0.7915 - val_loss: 1.9894 - val_acc: 0.6713\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.66413 to 0.67133, saving model to ./spanish-english.h5\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 0.9218 - acc: 0.8095 - val_loss: 1.9825 - val_acc: 0.6711\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.67133\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 0.8340 - acc: 0.8260 - val_loss: 1.9961 - val_acc: 0.6788\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.67133 to 0.67879, saving model to ./spanish-english.h5\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 0.7504 - acc: 0.8441 - val_loss: 2.0216 - val_acc: 0.6774\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.67879\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 0.6827 - acc: 0.8582 - val_loss: 2.0288 - val_acc: 0.6793\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.67879 to 0.67933, saving model to ./spanish-english.h5\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 9s 35ms/step - loss: 0.6190 - acc: 0.8701 - val_loss: 2.0342 - val_acc: 0.6770\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.67933\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 9s 35ms/step - loss: 0.5648 - acc: 0.8821 - val_loss: 2.0527 - val_acc: 0.6842\n",
            "\n",
            "Epoch 00019: val_acc improved from 0.67933 to 0.68421, saving model to ./spanish-english.h5\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 9s 36ms/step - loss: 0.5170 - acc: 0.8947 - val_loss: 2.0596 - val_acc: 0.6877\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.68421 to 0.68767, saving model to ./spanish-english.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:57.032153Z",
          "iopub.status.busy": "2021-01-21T13:36:57.031063Z",
          "iopub.status.idle": "2021-01-21T13:36:57.047282Z",
          "shell.execute_reply": "2021-01-21T13:36:57.046676Z"
        },
        "papermill": {
          "duration": 0.844983,
          "end_time": "2021-01-21T13:36:57.047395",
          "exception": false,
          "start_time": "2021-01-21T13:36:56.202412",
          "status": "completed"
        },
        "tags": [],
        "id": "jiNVZlvP4oCb"
      },
      "source": [
        "# loading the weights from the best saved model\n",
        "model.load_weights(filepath)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFknwSI64oCb"
      },
      "source": [
        "### Making the functions to predict the sequence and BLEU_sccore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:36:58.701542Z",
          "iopub.status.busy": "2021-01-21T13:36:58.699715Z",
          "iopub.status.idle": "2021-01-21T13:36:58.702231Z",
          "shell.execute_reply": "2021-01-21T13:36:58.702636Z"
        },
        "papermill": {
          "duration": 0.838898,
          "end_time": "2021-01-21T13:36:58.702743",
          "exception": false,
          "start_time": "2021-01-21T13:36:57.863845",
          "status": "completed"
        },
        "tags": [],
        "id": "N6QTt-LU4oCb"
      },
      "source": [
        "# a dictionary having key is a token number for a particular word and value is a word\n",
        "# this will required to decode the predicted sequence\n",
        "target_vocab_idx = {v:k for k,v in target_tokenizer.word_index.items()}\n",
        "\n",
        "# function to predict the decoded sequence\n",
        "def predict_sequence(model,sent,vocab_idx):\n",
        "    prediction = model.predict(sent.reshape(1,max_source_length))[0]\n",
        "    integers = [np.argmax(vector) for vector in prediction]\n",
        "    target = []\n",
        "    for i in integers:\n",
        "        if i != 0:\n",
        "            word = vocab_idx[i]\n",
        "            if word is None:\n",
        "                break\n",
        "            target.append(word)\n",
        "    return ' '.join(target)\n",
        "\n",
        "# for evaluation of the model through BLEU_score\n",
        "def bleu_score(model,ip,ip_raw,op_raw,vocab_idx):\n",
        "    prediction,actual = [],[]\n",
        "    for i,sent in enumerate(ip):\n",
        "        if i%10 == 0:\n",
        "            print('\\rprogress ',(i+1)*100//len(ip),'%',sep='',end='',flush = True)\n",
        "        translation = predict_sequence(model,sent,vocab_idx)\n",
        "        prediction.append(translation)\n",
        "        actual.append(op_raw[i])\n",
        "    print()\n",
        "    # printing the first ten sentences\n",
        "    for i in range(10):\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "        print('SPANISH -->',ip_raw[i],' || ','ACTUAL ENGLISH -->',op_raw[i],' || ','PREDICTED ENGLISH -->',prediction[i])\n",
        "        print(\"--------------------------------------------------------------------------------------------------------------------------------------------------\")\n",
        "    print()\n",
        "    # printing the BLEU_score\n",
        "    print(\"----------------------------------\")\n",
        "    print('Printing BLEU SCORE...')\n",
        "    print(\"----------------------------------\")\n",
        "    print('First BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(1.0, 0, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Second BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Third BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print('Fourth BLEU score --> %f' % corpus_bleu(actual, prediction, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothing,auto_reweigh=False))\n",
        "    print(\"----------------------------------\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksPcAEs24oCc"
      },
      "source": [
        "### Evaluating the model on training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:37:00.637318Z",
          "iopub.status.busy": "2021-01-21T13:37:00.636492Z",
          "iopub.status.idle": "2021-01-21T13:47:02.656690Z",
          "shell.execute_reply": "2021-01-21T13:47:02.656212Z"
        },
        "papermill": {
          "duration": 603.117053,
          "end_time": "2021-01-21T13:47:02.656787",
          "exception": false,
          "start_time": "2021-01-21T13:36:59.539734",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2KUCpGO4oCc",
        "outputId": "a0bf623d-2308-45e2-87ea-438c1f924fca"
      },
      "source": [
        "bleu_score(model,source_train_seq_pad,source_train,target_train,target_vocab_idx)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> cul es bueno  ||  ACTUAL ENGLISH --> which one is good  ||  PREDICTED ENGLISH --> which one is good\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> te gusta el colegio  ||  ACTUAL ENGLISH --> do you like school  ||  PREDICTED ENGLISH --> do you like school\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> soy gemelo  ||  ACTUAL ENGLISH --> i'm a twin  ||  PREDICTED ENGLISH --> i'm a twin\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> podemos rentar un coche  ||  ACTUAL ENGLISH --> can we rent a car  ||  PREDICTED ENGLISH --> can we we a car\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> me estoy desnudando  ||  ACTUAL ENGLISH --> i'm undressing  ||  PREDICTED ENGLISH --> i'm undressing\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> sé una buena chica  ||  ACTUAL ENGLISH --> be a good girl  ||  PREDICTED ENGLISH --> be a good girl\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> te acompao  ||  ACTUAL ENGLISH --> i'll accompany you  ||  PREDICTED ENGLISH --> i'll i'll for you\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> estoy harto del francés  ||  ACTUAL ENGLISH --> i'm sick of french  ||  PREDICTED ENGLISH --> i'm sick of french\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> ella debe estar enfadada  ||  ACTUAL ENGLISH --> she must be angry  ||  PREDICTED ENGLISH --> she must be angry\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> tom fue severo  ||  ACTUAL ENGLISH --> tom was harsh  ||  PREDICTED ENGLISH --> tom was harsh\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.660528\n",
            "Second BLEU score --> 0.462708\n",
            "Third BLEU score --> 0.412877\n",
            "Fourth BLEU score --> 0.318621\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DYxGyiZ4oCd"
      },
      "source": [
        "### Evaluating the model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-01-21T13:47:05.406595Z",
          "iopub.status.busy": "2021-01-21T13:47:05.405722Z",
          "iopub.status.idle": "2021-01-21T13:49:35.405960Z",
          "shell.execute_reply": "2021-01-21T13:49:35.405286Z"
        },
        "papermill": {
          "duration": 151.514089,
          "end_time": "2021-01-21T13:49:35.406089",
          "exception": false,
          "start_time": "2021-01-21T13:47:03.892000",
          "status": "completed"
        },
        "tags": [],
        "id": "oqw6Q6Fb4oCd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "652bc4ec-762d-4732-f1d4-1f02f54e5c67"
      },
      "source": [
        "bleu_score(model,source_test_seq_pad,source_test,target_test,target_vocab_idx)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress 99%\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> tengo una tos seca  ||  ACTUAL ENGLISH --> i have a dry cough  ||  PREDICTED ENGLISH --> i have a black eye\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> no es broma  ||  ACTUAL ENGLISH --> i kid you not  ||  PREDICTED ENGLISH --> isn't no joke\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> firme aqu por favor  ||  ACTUAL ENGLISH --> sign here please  ||  PREDICTED ENGLISH --> sign here please\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> le di un mapa  ||  ACTUAL ENGLISH --> i handed him a map  ||  PREDICTED ENGLISH --> i gave him a doctor\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> no soy espa  ||  ACTUAL ENGLISH --> i'm not a spy  ||  PREDICTED ENGLISH --> i'm i'm a member\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> pasa desapercibido  ||  ACTUAL ENGLISH --> lie low  ||  PREDICTED ENGLISH --> it's better again\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> tiene un perro  ||  ACTUAL ENGLISH --> he has a dog  ||  PREDICTED ENGLISH --> he has a dog\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> verifique su orden  ||  ACTUAL ENGLISH --> check your order  ||  PREDICTED ENGLISH --> here's your job\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> esta es una gran noticia  ||  ACTUAL ENGLISH --> this is big news  ||  PREDICTED ENGLISH --> this is is jumped\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "SPANISH --> es nuevo  ||  ACTUAL ENGLISH --> that's new  ||  PREDICTED ENGLISH --> it's new\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "----------------------------------\n",
            "Printing BLEU SCORE...\n",
            "----------------------------------\n",
            "First BLEU score --> 0.542889\n",
            "Second BLEU score --> 0.445948\n",
            "Third BLEU score --> 0.415013\n",
            "Fourth BLEU score --> 0.325841\n",
            "----------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}